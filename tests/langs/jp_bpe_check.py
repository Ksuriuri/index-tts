import os
import sys
import datetime

# å°è¯•æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(root_dir)

from collections import Counter
from tqdm import tqdm
from modelscope.msdatasets import MsDataset
# å‡è®¾ indextts å·²ç»åœ¨ä½ çš„ç¯å¢ƒä¸­å®‰è£…æˆ–ä½äºè·¯å¾„ä¸­
try:
    from indextts.utils.front import TextNormalizer, TextTokenizer
except ImportError:
    print("æç¤º: æœªæ‰¾åˆ° indextts åº“ï¼Œè¯·ç¡®ä¿ä»£ç è¿è¡Œåœ¨æ­£ç¡®çš„ç¯å¢ƒä¸­ã€‚")
    # ä¸ºäº†é˜²æ­¢ç›´æ¥æŠ¥é”™é€€å‡ºï¼Œè¿™é‡Œåªåšæç¤ºï¼Œå®é™…è¿è¡Œæ—¶å¦‚æœç¼ºåº“ä¼šæŠ¥é”™
    pass

# --- é…ç½®éƒ¨åˆ† ---
MODELSCOPE_CACHE_DIR = r'./outputs'
MODEL_FILE = r"D:\workspace\index-tts\checkpoints\IndexTTS-2-vLLM\jp_bpe.model"  # è¯·ä¿®æ”¹ä¸ºä½ çš„å®é™…è·¯å¾„
UNK_ID = 2
TEST_SAMPLE_COUNT = 10000  # æµ‹è¯•å¤šå°‘æ¡æ•°æ®
DATASET_ID = 'wikimedia/wikipedia'
SUBSET_NAME = '20231101.ja' 

# ç»“æœä¿å­˜è·¯å¾„
RESULT_SAVE_PATH = r'./outputs/test_report.txt'
UNK_DETAILS_PATH = r'./outputs/unk_tokens_details.csv'

def run_modelscope_test():
    # --- 1. åŠ è½½ä½ çš„ BPE æ¨¡å‹ ---
    print(f">>> æ­£åœ¨åŠ è½½ BPE æ¨¡å‹: {MODEL_FILE}")
    if not os.path.exists(MODEL_FILE):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {MODEL_FILE}")
        return

    try:
        normalizer = TextNormalizer()
        normalizer.load()
        tokenizer = TextTokenizer(MODEL_FILE, normalizer)
    except Exception as e:
        print(f"æ¨¡å‹åŠ è½½å‡ºé”™: {e}")
        return

    # --- 2. è¿æ¥ ModelScope åŠ è½½æ•°æ® (æµå¼) ---
    print(f">>> æ­£åœ¨è¿æ¥ ModelScope (å›½å†…æº)...")
    print(f">>> ç¼“å­˜ç›®å½•å·²è®¾ç½®ä¸º: {MODELSCOPE_CACHE_DIR}")
    
    try:
        ds = MsDataset.load(
            DATASET_ID, 
            subset_name=SUBSET_NAME, 
            split='train', 
            use_streaming=True,
            cache_dir=MODELSCOPE_CACHE_DIR
        )
    except Exception as e:
        print(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return

    print(f">>> å¼€å§‹æµ‹è¯•å‰ {TEST_SAMPLE_COUNT} æ¡æ ·æœ¬...")

    # --- 3. å¾ªç¯æµ‹è¯• ---
    total_tokens = 0
    total_unks = 0
    unk_counter = Counter()
    processed_count = 0
    
    ds_iter = iter(ds)
    progress_bar = tqdm(total=TEST_SAMPLE_COUNT, desc="Processing", unit="samples")

    while processed_count < TEST_SAMPLE_COUNT:
        try:
            item = next(ds_iter)
        except StopIteration:
            break

        text = item.get('text', '')
        if not text:
            continue

        # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬ä»¥æé«˜é€Ÿåº¦
        text = text[:500]

        try:
            # Tokenize
            tokens = tokenizer.tokenize(text)
            ids = tokenizer.convert_tokens_to_ids(tokens)
            
            # ç»Ÿè®¡
            batch_unks = ids.count(UNK_ID)
            total_tokens += len(ids)
            total_unks += batch_unks
            
            # è®°å½•å…·ä½“çš„ UNK
            if batch_unks > 0:
                for t_str, t_id in zip(tokens, ids):
                    if t_id == UNK_ID:
                        unk_counter[t_str] += 1
            
            processed_count += 1
            progress_bar.update(1)
            
        except Exception as e:
            pass 

    progress_bar.close()

    # --- 4. ç»“æœç”Ÿæˆä¸ä¿å­˜ ---
    if total_tokens == 0:
        print("æœªå¤„ç†ä»»ä½•æ•°æ®ã€‚")
        return

    coverage = (1 - (total_unks / total_tokens)) * 100
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # æ„å»ºæŠ¥å‘Šå†…å®¹å­—ç¬¦ä¸²
    lines = []
    lines.append("="*60)
    lines.append(f"ModelScope æµ‹è¯•æŠ¥å‘Š")
    lines.append(f"æµ‹è¯•æ—¶é—´:   {current_time}")
    lines.append("="*60)
    lines.append(f"æ¨¡å‹è·¯å¾„:   {MODEL_FILE}")
    lines.append(f"æ•°æ®é›†:     {DATASET_ID} ({SUBSET_NAME})")
    lines.append(f"æ ·æœ¬æ•°é‡:   {processed_count}")
    lines.append("-" * 60)
    lines.append(f"æ€» Token:   {total_tokens}")
    lines.append(f"æ€» UNK:     {total_unks}")
    lines.append(f"è¦†ç›–ç‡:     {coverage:.4f}%")
    lines.append("="*60)

    lines.append("\n>>> â˜ ï¸  å¯¼è‡´ UNK æœ€å¤šçš„å‰ 50 ä¸ª Token:")
    if not unk_counter:
        lines.append("æ—  (å®Œç¾ï¼)")
    else:
        for token, count in unk_counter.most_common(50):
            # ä¸ºäº†é˜²æ­¢ç‰¹æ®Šå­—ç¬¦ç ´åæ’ç‰ˆï¼Œä½¿ç”¨ repr
            lines.append(f"  {repr(token):<20} : {count} æ¬¡")

    lines.append("\n>>> ğŸš€ ç»“è®º:")
    if coverage > 99.0:
        lines.append("âœ… å¼ºï¼šè¿™ä¸ªæ¨¡å‹æ—¥è¯­èƒ½åŠ›å¾ˆæ£’ã€‚")
    elif coverage > 95.0:
        lines.append("ğŸ†— ä¸­ï¼šå¤§éƒ¨åˆ†èƒ½è¯»ï¼Œä½†å¯èƒ½æœ‰ä¸€äº›ç‰¹å®šæ±‰å­—ä¸è¡Œã€‚")
    else:
        lines.append("âŒ å¼±ï¼šè¿™å¯èƒ½ä¸æ˜¯ä¸€ä¸ªåˆæ ¼çš„æ—¥è¯­æ¨¡å‹ï¼Œæˆ–è€…è¯è¡¨ä¸»è¦é’ˆå¯¹ä¸­æ–‡ã€‚")

    report_content = "\n".join(lines)

    # 1. æ‰“å°åˆ°æ§åˆ¶å°
    print("\n" + report_content)

    # 2. ä¿å­˜æŠ¥å‘Šåˆ° TXT
    try:
        with open(RESULT_SAVE_PATH, 'w', encoding='utf-8') as f:
            f.write(report_content)
        print(f"\n[OK] æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜è‡³: {os.path.abspath(RESULT_SAVE_PATH)}")
    except Exception as e:
        print(f"\n[Error] ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

    # 3. (å¯é€‰) ä¿å­˜è¯¦ç»†çš„ UNK åˆ—è¡¨åˆ° CSVï¼Œæ–¹ä¾¿åç»­åˆ†æ
    if unk_counter:
        try:
            with open(UNK_DETAILS_PATH, 'w', encoding='utf-8-sig') as f:
                f.write("Token,Count\n")
                for token, count in unk_counter.most_common():
                    # å¤„ç† CSV ä¸­çš„é€—å·å’Œæ¢è¡Œ
                    clean_token = token.replace('"', '""')
                    f.write(f'"{clean_token}",{count}\n')
            print(f"[OK] è¯¦ç»† UNK åˆ—è¡¨å·²ä¿å­˜è‡³: {os.path.abspath(UNK_DETAILS_PATH)}")
        except Exception as e:
            print(f"[Error] ä¿å­˜ UNK è¯¦æƒ…å¤±è´¥: {e}")

if __name__ == "__main__":
    run_modelscope_test()